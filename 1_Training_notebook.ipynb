{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeoKauko/TheNavySeals/blob/main/1_Training_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SealNN training notebook\n",
        "This notebook contains the image processing of MAXAR satellite images and the training of the deep learning.\n",
        "The input requires the satellite image as .tif and seal annotations as .shp. The output is a the SealNN model that will be used in detecting seals in the following post-processing notebook."
      ],
      "metadata": {
        "id": "Rvwz6WwMQT73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import libraries"
      ],
      "metadata": {
        "id": "u6aaHDKWQx81"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Google colab or connect to a local drive"
      ],
      "metadata": {
        "id": "2kAyYEKRRFFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Set up working directory and project directories\n",
        "\n",
        "def in_colab():\n",
        "    \"\"\"\n",
        "    Check if the code is running in Google Colab.\n",
        "\n",
        "    Args:\n",
        "    None\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the code is running in Google Colab, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except ImportError:\n",
        "        return False\n",
        "\n",
        "import os\n",
        "\n",
        "## Set working directory to Google Drive or Local based on usage\n",
        "if in_colab():\n",
        "    ## GOOGLE COLAB USERS ONLY\n",
        "    ## Mount Google Drive for data retrieval\n",
        "    ## This cell assumes that you have copied the \"TheNavySeals\" GitHub repository into your root drive folder\n",
        "    print(\"Running in Google Colab\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    project_path = '/content/drive/MyDrive/TheNavySeals/'\n",
        "\n",
        "    !pip install torch torchvision segmentation-models-pytorch tifffile tabulate rasterio -q\n",
        "else:\n",
        "    ## LOCAL USERS ONLY\n",
        "    ## Change the path to your project directory (where you cloned the GitHub repository)\n",
        "    ## If you are running local, make sure you have set up the environment according to the readme in GitHub\n",
        "    print(\"Running Locally\")\n",
        "    os.chdir('C:/TheNavySeals')\n",
        "    project_path = ''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zabw1cPbas-O",
        "outputId": "085b8882-ebf1-435b-cc15-b57ae83f4779"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required packages and libraries"
      ],
      "metadata": {
        "id": "UGGDhrMXQ7uV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyKx_RuY9L7f"
      },
      "source": [
        "## Import libraries for image processing\n",
        "import os\n",
        "from os import path\n",
        "import os.path\n",
        "from osgeo import gdal\n",
        "import rasterio\n",
        "from rasterio import windows\n",
        "from rasterio.windows import Window\n",
        "from rasterio.plot import reshape_as_image\n",
        "import rasterio.mask\n",
        "from rasterio.features import rasterize, geometry_mask\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import mapping, Point, box, Polygon\n",
        "from shapely.ops import cascaded_union\n",
        "import numpy as np\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
        "from glob import glob\n",
        "from google.colab import drive\n",
        "\n",
        "## Import libraries for training\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision.transforms import v2 as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import segmentation_models_pytorch as smp\n",
        "import matplotlib.pyplot as plt\n",
        "import tifffile\n",
        "from PIL import Image\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "import random"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions for image processing"
      ],
      "metadata": {
        "id": "gfUci_LfbB7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_radiometric_resolution(input_path, output_path, input_res=11):\n",
        "    \"\"\"\n",
        "    Reduces the radiometric resolution of the input raster and save the output raster.\n",
        "\n",
        "    Args:\n",
        "    - input_path (string): Path to the input raster.\n",
        "    - output_path (string): Path to the output raster.\n",
        "    - input_res (int): Radiometric resolution of the input raster in bits.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure the output directory exists\n",
        "    output_dir = os.path.dirname(output_path)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    with rasterio.open(input_path) as src:\n",
        "        # Read the number of bands\n",
        "        num_bands = src.count\n",
        "\n",
        "        # Initialize an array to store the scaled bands\n",
        "        scaled_arrays = []\n",
        "\n",
        "        for band in range(1, num_bands + 1):\n",
        "            # Read the image band as a numpy array\n",
        "            image_array = src.read(band, masked=True)\n",
        "\n",
        "            # Rescale the pixel values to fit within 8-bit range (0-255)\n",
        "            scaled_array = (image_array / (2**input_res - 1) * 255).astype(np.uint8)\n",
        "\n",
        "            # Append the scaled array to the list\n",
        "            scaled_arrays.append(scaled_array)\n",
        "\n",
        "        # Stack the scaled arrays along the first axis to create a 3D array\n",
        "        scaled_arrays = np.stack(scaled_arrays, axis=0)\n",
        "\n",
        "        # Create a new raster profile with 8-bit pixel depth\n",
        "        profile = src.profile\n",
        "        profile.update(dtype=rasterio.uint8, count=num_bands)\n",
        "\n",
        "        # Write the scaled arrays to a new raster file\n",
        "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
        "            dst.write(scaled_arrays)\n",
        "\n",
        "\n",
        "\n",
        "def mosaic_rasters(input_folder, output_path):\n",
        "    \"\"\"\n",
        "    Mosaics multiple multiband rasters together.\n",
        "\n",
        "    Args:\n",
        "    - input_folder (string): Path to the folder containing the rasters.\n",
        "    - output_path (string): Path to the output raster.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # List to hold the file paths of the rasters to be merged\n",
        "    input_files = []\n",
        "\n",
        "    # Loop through the folder and add all .tif files to the list\n",
        "    for file_name in os.listdir(input_folder):\n",
        "        print(file_name)\n",
        "        if file_name.endswith('.TIF'):\n",
        "            input_files.append(os.path.join(input_folder, file_name))\n",
        "\n",
        "    # Check if we have any input files\n",
        "    if not input_files:\n",
        "        raise FileNotFoundError(\"No .tif files found in the specified folder.\")\n",
        "\n",
        "    # Open the input files\n",
        "    src_files_to_mosaic = []\n",
        "    for file in input_files:\n",
        "        src = gdal.Open(file)\n",
        "        if src:\n",
        "            src_files_to_mosaic.append(src)\n",
        "        else:\n",
        "            print(f\"Failed to open {file}\")\n",
        "\n",
        "    # Create a virtual raster from the input files\n",
        "    vrt = gdal.BuildVRT('temporary.vrt', src_files_to_mosaic)\n",
        "\n",
        "    # Write the virtual raster to a new file\n",
        "    gdal.Translate(output_path, vrt)\n",
        "\n",
        "    # Cleanup\n",
        "    vrt = None\n",
        "    for src in src_files_to_mosaic:\n",
        "        src = None\n",
        "\n",
        "    print(f\"Mosaic raster saved as {output_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def create_polygon_from_pixels(row, col, transform):\n",
        "    \"\"\"\n",
        "    Creates a polygon from raster pixels.\n",
        "\n",
        "    Args:\n",
        "    - row (int): The row index of the center pixel.\n",
        "    - col (int): The column index of the center pixel.\n",
        "    - transform (tuple): A tuple of 6 elements representing the affine transformation coefficients\n",
        "                         (a, b, c, d, e, f) which map pixel coordinates (x, y) to geographic coordinates (X, Y):\n",
        "                         X = a + b * x + c * y\n",
        "                         Y = d + e * x + f * y\n",
        "\n",
        "    Returns:\n",
        "    - Polygon: A polygon representing a 5x5 block of pixels centered around the given pixel.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate the coordinates of the top-left corner of the top-left pixel\n",
        "    top_left_x = transform[0] + (col - 2) * transform[1] + (row - 2) * transform[2]\n",
        "    top_left_y = transform[3] + (col - 2) * transform[4] + (row - 2) * transform[5]\n",
        "\n",
        "    # Pixel dimensions\n",
        "    pixel_width = abs(transform[1])\n",
        "    pixel_height = abs(transform[5])\n",
        "\n",
        "    # Calculate the coordinates for the 5x5 block of pixels\n",
        "    polygon_coords = [\n",
        "        (top_left_x, top_left_y),\n",
        "        (top_left_x + 5 * pixel_width, top_left_y),\n",
        "        (top_left_x + 5 * pixel_width, top_left_y - 5 * pixel_height),\n",
        "        (top_left_x, top_left_y - 5 * pixel_height),\n",
        "        (top_left_x, top_left_y)\n",
        "    ]\n",
        "\n",
        "    return Polygon(polygon_coords)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def raster_points_to_polygons(raster_path, shapefile_path):\n",
        "    \"\"\"\n",
        "    Converts point geometries in a shapefile to polygons representing 5x5 pixel blocks in a raster.\n",
        "\n",
        "    Args:\n",
        "    - raster_path (str): File path to the raster dataset.\n",
        "    - shapefile_path (str): File path to the shapefile with point geometries.\n",
        "\n",
        "    Returns:\n",
        "    geopandas.GeoDataFrame: GeoDataFrame with polygons representing 5x5 pixel blocks centered around points in the shapefile.\n",
        "    \"\"\"\n",
        "\n",
        "    # Read raster data\n",
        "    raster_dataset = gdal.Open(raster_path)\n",
        "    raster_geotransform = raster_dataset.GetGeoTransform()\n",
        "\n",
        "    # Read shapefile\n",
        "    shapefile_gdf = gpd.read_file(shapefile_path)\n",
        "    shape_crs = shapefile_gdf.crs\n",
        "\n",
        "    polygons = []\n",
        "    for point in shapefile_gdf.geometry:\n",
        "        # Convert point coordinates to raster coordinates\n",
        "        x, y = point.x, point.y\n",
        "        col = int((x - raster_geotransform[0]) / raster_geotransform[1])\n",
        "        row = int((y - raster_geotransform[3]) / raster_geotransform[5])\n",
        "\n",
        "        # Create polygon around the pixel and its 24 surrounding pixels\n",
        "        polygon = create_polygon_from_pixels(row, col, raster_geotransform)\n",
        "        polygons.append(polygon)\n",
        "\n",
        "    result_gdf = gpd.GeoDataFrame(geometry=polygons, crs=shape_crs)\n",
        "\n",
        "    return result_gdf\n",
        "\n",
        "def mask_raster_with_polygon(input_raster_path, polygons, output_raster_path, value=1):\n",
        "    \"\"\"\n",
        "    Creates a copy of a raster, set all its values to 0, overlay it with a polygon shapefile,\n",
        "    and set all pixels underneath polygons to a specified value.\n",
        "\n",
        "    Args:\n",
        "    - input_raster_path (str): Path to the input raster.\n",
        "    - polygons (gdf): GeoDataFrame with the polygons of the mask.\n",
        "    - output_raster_path (str): Path to save the masked raster.\n",
        "    - value (int, optional): Value to set for pixels underneath polygons. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Open the input raster for reading\n",
        "    with rasterio.open(input_raster_path) as src:\n",
        "        # Read raster data\n",
        "        raster_data = src.read(1)\n",
        "        # Get metadata\n",
        "        meta = src.meta\n",
        "\n",
        "    # Set all values to 0\n",
        "    raster_data.fill(0)\n",
        "\n",
        "    # Create mask from polygons\n",
        "    mask = geometry_mask(polygons.geometry, out_shape=raster_data.shape, transform=src.transform, invert=True)\n",
        "\n",
        "    # Set pixels underneath polygons to the specified value\n",
        "    raster_data[mask] = value\n",
        "\n",
        "    # Save the masked raster\n",
        "    with rasterio.open(output_raster_path, 'w', **meta) as dst:\n",
        "        dst.write(raster_data, 1)\n",
        "\n",
        "\n",
        "def tile_raster(input_raster_path, part_width, part_height, output_folder):\n",
        "    \"\"\"\n",
        "    Splits a raster into multiple tiles of length part_width and height part_height, and save them in output_folder.\n",
        "\n",
        "    Args:\n",
        "    - input_raster_path: path to the input raster.\n",
        "    - part_width (int): Width of each tile.\n",
        "    - part_height (int): Height of each tile.\n",
        "    - output_folder (str): Directory to save the rasters.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Open the raster\n",
        "    dataset = gdal.Open(input_raster_path)\n",
        "\n",
        "    # Get raster dimensions\n",
        "    width = dataset.RasterXSize\n",
        "    height = dataset.RasterYSize\n",
        "\n",
        "    # Calculate the number of parts\n",
        "    num_parts_x = width // part_width\n",
        "    num_parts_y = height // part_height\n",
        "\n",
        "    # Get the number of bands\n",
        "    bands = dataset.RasterCount\n",
        "\n",
        "    # Split the raster and save\n",
        "    for i in range(num_parts_x):\n",
        "        for j in range(num_parts_y):\n",
        "            x_offset = i * part_width\n",
        "            y_offset = j * part_height\n",
        "\n",
        "            # Read the split region\n",
        "            part = dataset.ReadAsArray(x_offset, y_offset, part_width, part_height)\n",
        "\n",
        "            # Expand dimensions if there's only one band\n",
        "            if bands == 1:\n",
        "               part = np.expand_dims(part, axis=0)\n",
        "\n",
        "            # Create a new GDAL dataset to save the split part\n",
        "            driver = gdal.GetDriverByName('GTiff')\n",
        "            output_path = os.path.join(output_folder, f'part_{i}_{j}.tif')\n",
        "            out_dataset = driver.Create(output_path, part_width, part_height, bands, gdal.GDT_UInt16)\n",
        "\n",
        "            # Write data to the new dataset\n",
        "            for band in range(bands):\n",
        "                out_band = out_dataset.GetRasterBand(band + 1)\n",
        "                out_band.WriteArray(part[band])\n",
        "\n",
        "            # Set georeference and projection\n",
        "            geo_transform = list(dataset.GetGeoTransform())\n",
        "            geo_transform[0] += x_offset * geo_transform[1]\n",
        "            geo_transform[3] += y_offset * geo_transform[5]\n",
        "            out_dataset.SetGeoTransform(tuple(geo_transform))\n",
        "            out_dataset.SetProjection(dataset.GetProjection())\n",
        "\n",
        "            # Save and close\n",
        "            out_dataset.FlushCache()\n",
        "            del out_dataset\n",
        "\n",
        "    # Close the original dataset\n",
        "    del dataset\n",
        "\n",
        "\n",
        "\n",
        "def contains_zero(input_raster_path):\n",
        "    \"\"\"\n",
        "    Checks if a single band raster has a zero value pixel.\n",
        "\n",
        "    Args:\n",
        "    - input_raster_path (str): Path to the input raster.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the raster contains at least one zero value pixel, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    with rasterio.open(input_raster_path) as src:\n",
        "        # Read the image as a numpy array\n",
        "        raster_array = src.read(1)\n",
        "        # Check if the array contains any zero values\n",
        "        return (raster_array == 0).any()\n",
        "\n",
        "\n",
        "def remove_images_with_zero_panchromatic(directory, panchromatic_directory, mask_directory):\n",
        "    \"\"\"\n",
        "    Removes images from a directory if they contain a zero value pixel in their corresponding panchromatic image.\n",
        "    Also removes corresponding images from the mask directory.\n",
        "\n",
        "    Args:\n",
        "    - directory (str): Path to the directory containing the images and masks to validate.\n",
        "    - panchromatic_directory (str): Path to the directory containing the panchromatic images.\n",
        "    - mask_directory (str): Path to the directory containing the mask images.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(('.tif', '.tiff')):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            panchromatic_path = os.path.join(panchromatic_directory, filename)\n",
        "            if contains_zero(panchromatic_path):\n",
        "                os.remove(file_path)\n",
        "                print(f\"Removed image: {file_path}\")\n",
        "\n",
        "                # Remove the mask image with the same name\n",
        "                mask_path = os.path.join(mask_directory, filename)\n",
        "                if os.path.exists(mask_path):\n",
        "                    os.remove(mask_path)\n",
        "                    print(f\"Removed mask: {mask_path}\")\n",
        "\n",
        "\n",
        "                # Remove the pansharpened image with the same name\n",
        "                panchromatic_path = os.path.join(panchromatic_directory, filename)\n",
        "                if os.path.exists(panchromatic_path):\n",
        "                    os.remove(panchromatic_path)\n",
        "                    print(f\"Removed mask: {panchromatic_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def contains_zero(input_raster_path):\n",
        "    \"\"\"\n",
        "    Checks if a single band raster has a zero value pixel.\n",
        "\n",
        "    Args:\n",
        "    - input_raster_path (str): Path to the input raster.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if the raster contains at least one zero value pixel, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    with rasterio.open(input_raster_path) as src:\n",
        "        # Read the image as a numpy array\n",
        "        raster_array = src.read(1)\n",
        "        # Check if the array contains any zero values\n",
        "        return (raster_array == 0).any()\n",
        "\n",
        "\n",
        "def contains_zero_multiband(image_path):\n",
        "    \"\"\"\n",
        "    Check if an image has a zero value pixel in any of its bands.\n",
        "\n",
        "    Args:\n",
        "    - image_path (str): File path to the multiband image file.\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if any band in the image contains at least one zero value pixel, False otherwise.\n",
        "    \"\"\"\n",
        "\n",
        "    with rasterio.open(image_path) as src:\n",
        "        # Iterate through each band\n",
        "        for band in range(1, src.count + 1):\n",
        "            # Read the current band as a numpy array\n",
        "            image_array = src.read(band)\n",
        "            # Check if the array contains any zero values\n",
        "            if (image_array == 0).any():\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "def remove_mulraster_with_zero_values(directory):\n",
        "    \"\"\"\n",
        "    Removes raster files from a directory if they contain any zero value pixels.\n",
        "\n",
        "    Args:\n",
        "    - directory (str): Path to the directory containing the raster images to validate and potentially remove.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(('.tif', '.tiff')):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            if contains_zero_multiband(file_path):\n",
        "                os.remove(file_path)\n",
        "                #print(f\"Removed raster: {file_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def remove_zero_raster_mask(directory, mask_directory):\n",
        "    \"\"\"\n",
        "    Removes all rasters from a directory that contain a zero value pixel.\n",
        "    Also removes rasters with the same name from the mask directory.\n",
        "\n",
        "    Args:\n",
        "    - directory (str): Path to the directory containing the rasters to validate.\n",
        "    - mask_directory (str): Name of the folder containing the mask rasters with the same name.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(('.tif', '.tiff')):\n",
        "            file_path = os.path.join(directory, filename)\n",
        "            if contains_zero(file_path):\n",
        "                os.remove(file_path)\n",
        "                # print(f\"Removed image: {file_path}\")\n",
        "\n",
        "                # Remove the image with the same name from the similar folder\n",
        "                mask_path = os.path.join(mask_directory, filename)\n",
        "                if os.path.exists(mask_path):\n",
        "                    os.remove(mask_path)\n",
        "                    # print(f\"Removed similar image: {mask_path}\")\n",
        "\n",
        "\n",
        "\n",
        "def update_masks(panchromatic_parts_path, mask_parts_path, threshold_value):\n",
        "    \"\"\"\n",
        "    Updates mask images based on corresponding panchromatic images. Specifically, for each mask,\n",
        "    all pixels with a value of 1 that correspond to pixels in the panchromatic image with a value above the\n",
        "    specified threshold are set to 0 in the new mask.\n",
        "\n",
        "    Args:\n",
        "    - panchromatic_parts_path (str): The directory path containing the panchromatic images.\n",
        "    - mask_parts_path (str): The directory path containing the mask images.\n",
        "    - threshold_value (int): The threshold value for the panchromatic image pixels. Pixels in the mask with a value of 1 and corresponding panchromatic image pixels above this threshold will be set to 0 in the new mask.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # List all mask files in the specified directory\n",
        "    mask_files = [f for f in os.listdir(mask_parts_path) if f.endswith('.tif')]\n",
        "\n",
        "    for mask_file in mask_files:\n",
        "        # Construct the full file paths for the mask and corresponding panchromatic image\n",
        "        mask_path = os.path.join(mask_parts_path, mask_file)\n",
        "        image_path = os.path.join(panchromatic_parts_path, mask_file)\n",
        "\n",
        "        # Read the panchromatic image\n",
        "        with rasterio.open(image_path) as img:\n",
        "            image_data = img.read(1)\n",
        "\n",
        "        # Read the mask image\n",
        "        with rasterio.open(mask_path) as mask:\n",
        "            mask_data = mask.read(1)\n",
        "            mask_meta = mask.meta\n",
        "\n",
        "        # Update the mask data based on the condition\n",
        "        new_mask_data = np.where((mask_data == 1) & (image_data > threshold_value), 0, mask_data)\n",
        "\n",
        "        # Save the new mask data overwriting the old mask\n",
        "        with rasterio.open(mask_path, 'w', **mask_meta) as mask:\n",
        "            mask.write(new_mask_data, 1)\n",
        "\n",
        "\n",
        "\n",
        "def obtain_statistic(image_path, shapefile_path):\n",
        "    \"\"\"\n",
        "    Counts the points that are within the bounds of an image and calculate the average pixel value.\n",
        "\n",
        "    Args:\n",
        "    - image_path (str): Path to the input raster.\n",
        "    - shapefile_path (str): Path to the shapefile containing points.\n",
        "\n",
        "    Returns:\n",
        "    - num_points (int): Number of points within the image bounds.\n",
        "    - avg_pixel_value (float): Average pixel value of the image.\n",
        "    \"\"\"\n",
        "\n",
        "    with rasterio.open(image_path) as src:\n",
        "        image_bounds = src.bounds\n",
        "        image_box = box(image_bounds.left, image_bounds.bottom, image_bounds.right, image_bounds.top)\n",
        "        # Read the CRS from the image\n",
        "        image_crs = src.crs\n",
        "        # Calculate the average pixel value\n",
        "        image_data = src.read(1)  # Read the first band\n",
        "        avg_pixel_value = np.mean(image_data)\n",
        "    shapefile = gpd.read_file(shapefile_path)\n",
        "\n",
        "    shapefile['within_image'] = shapefile.apply(lambda row: image_box.contains(Point(row.geometry.x, row.geometry.y)), axis=1)\n",
        "    points_within_image = shapefile[shapefile['within_image']]\n",
        "\n",
        "    return len(points_within_image), avg_pixel_value\n",
        "\n",
        "\n",
        "def obtain_statistics(image_dir, shapefile_path, csv_path):\n",
        "    \"\"\"\n",
        "    Computes the statistics based on the obtain_statistic function above. This outputs the number of seals and average pixel value per image to a CSV file.\n",
        "\n",
        "    Args:\n",
        "    - image_dir(str): Image directory where the images to calculate for are located.\n",
        "    - shapefile_path(str): Path to the shapefile containing the points.\n",
        "    - csv_path(str): Path to the csv to store the results.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # List to store results\n",
        "    results = []\n",
        "\n",
        "    # Iterate through all images in the directory\n",
        "    for image_name in os.listdir(image_dir):\n",
        "        if image_name.endswith('.tif'):\n",
        "            image_path = os.path.join(image_dir, image_name)\n",
        "            num_points, avg_pixel_value = obtain_statistic(image_path, shapefile_path)\n",
        "            results.append({'image_name': image_name, 'num_points': num_points, 'avg_pixel_value': avg_pixel_value})\n",
        "            print(f'appended image {image_name}')\n",
        "    # Convert results to DataFrame and save as CSV\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(f\"Results saved to {csv_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def organize_images(csv_file, image_folder, output_folder):\n",
        "    \"\"\"\n",
        "    Organises images into subfolders based on the number of seals and average pixel values.\n",
        "\n",
        "    Args:\n",
        "    - csv_file (str): Path to the CSV file containing image data.\n",
        "    - image_folder (str): Directory containing the images.\n",
        "    - output_folder (str): Directory to save the organized subfolders.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Create the main output directories\n",
        "    no_seals_folder = os.path.join(output_folder, 'no_seals')\n",
        "    seals_folder = os.path.join(output_folder, 'seals')\n",
        "    os.makedirs(no_seals_folder, exist_ok=True)\n",
        "    os.makedirs(seals_folder, exist_ok=True)\n",
        "\n",
        "    # Create subfolders for 'no_seals'\n",
        "    ice_folder = os.path.join(no_seals_folder, 'ice')\n",
        "    water_folder = os.path.join(no_seals_folder, 'water')\n",
        "    os.makedirs(ice_folder, exist_ok=True)\n",
        "    os.makedirs(water_folder, exist_ok=True)\n",
        "\n",
        "    # Process each row in the CSV\n",
        "    for index, row in df.iterrows():\n",
        "        image_name = row[0]\n",
        "        seal_count = row[1]\n",
        "        avg_pixel_value = row[2]\n",
        "\n",
        "        # Define the source and destination paths\n",
        "        src_path = os.path.join(image_folder, image_name)\n",
        "\n",
        "        # Determine the destination folder based on seal count and pixel value\n",
        "        if seal_count == 0:\n",
        "            if avg_pixel_value > 20:\n",
        "                dst_folder = ice_folder\n",
        "            else:\n",
        "                dst_folder = water_folder\n",
        "        else:\n",
        "            dst_folder = seals_folder\n",
        "\n",
        "        # Copy the image to the appropriate folder\n",
        "        dst_path = os.path.join(dst_folder, image_name)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "    print(\"Images have been organized into subfolders.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def split_data(input_panchromatic, input_mask, output_path, train_ratio=0.8, val_ratio=0.15, test_ratio=0.05):\n",
        "    \"\"\"\n",
        "    Splits panchromatic images and corresponding masks into training, validation, and test sets based on specified ratios.\n",
        "\n",
        "    Args:\n",
        "    - input_panchromatic (str): Path to directory with panchromatic images.\n",
        "    - input_mask (str): Path to directory with corresponding mask images.\n",
        "    - output_path (str): Path where split dataset will be saved.\n",
        "    - train_ratio (float, optional): Ratio of training data (default is 0.8).\n",
        "    - val_ratio (float, optional): Ratio of validation data (default is 0.15).\n",
        "    - test_ratio (float, optional): Ratio of test data (default is 0.05).\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    assert train_ratio + val_ratio + test_ratio == 1, \"The ratios must sum to 1.\"\n",
        "    def move_files(files, output_subfolder):\n",
        "        for file in files:\n",
        "            filename = os.path.basename(file)\n",
        "            shutil.copy(file, os.path.join(output_subfolder, 'images', filename))\n",
        "            mask_file = file.replace(input_panchromatic, input_mask)\n",
        "            shutil.copy(mask_file, os.path.join(output_subfolder, 'masks', filename))\n",
        "    # Collect and sort images\n",
        "    seals_images = sorted(glob(os.path.join(input_panchromatic, 'seals', '*.tif')))\n",
        "    water_images = sorted(glob(os.path.join(input_panchromatic, 'no_seals', 'water', '*.tif')))\n",
        "    ice_images = sorted(glob(os.path.join(input_panchromatic, 'no_seals', 'ice', '*.tif')))\n",
        "    total_seals = len(seals_images)\n",
        "    half_seals = total_seals // 2\n",
        "\n",
        "    # Ensure water and ice have enough images\n",
        "    water_images = water_images[:half_seals]\n",
        "    ice_images = ice_images[:half_seals]\n",
        "    train_count = int(train_ratio * total_seals)\n",
        "    val_count = int(val_ratio * total_seals)\n",
        "    test_count = total_seals - train_count - val_count\n",
        "\n",
        "    # Split images\n",
        "    train_images = seals_images[:train_count] + water_images[:train_count//2] + ice_images[:train_count//2]\n",
        "    val_images = seals_images[train_count:train_count + val_count] + water_images[train_count//2:train_count//2 + val_count//2] + ice_images[train_count//2:train_count//2 + val_count//2]\n",
        "    test_images = seals_images[train_count + val_count:] + water_images[train_count//2 + val_count//2:] + ice_images[train_count//2 + val_count//2:]\n",
        "\n",
        "    move_files(train_images, os.path.join(output_path, 'train'))\n",
        "    move_files(val_images, os.path.join(output_path, 'val'))\n",
        "    move_files(test_images, os.path.join(output_path, 'test'))\n"
      ],
      "metadata": {
        "id": "ccVmVzsg9VQ3"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions for SealNN training"
      ],
      "metadata": {
        "id": "o76fmYuGevvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trans(image, mask):\n",
        "    \"\"\"\n",
        "    Transforms an image and its corresponding mask for use in a machine learning model training.\n",
        "\n",
        "    Args:\n",
        "    - image (PIL.Image.Image): Input image to be transformed.\n",
        "    - mask (PIL.Image.Image): Corresponding mask image to be transformed.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Transformed image tensor normalized and optionally augmented.\n",
        "    - torch.Tensor: Transformed mask tensor, possibly augmented to match the image transformation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert to tensor\n",
        "    image = TF.to_tensor(image).float() / 255.0 # get pixel values between 0 and 1 for uint8\n",
        "    mask = TF.to_tensor(mask).float() # Ensure mask is also in float32\n",
        "\n",
        "    # Set a random seed\n",
        "    seed = random.randint(0, 2**32 - 1)\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Apply horizontal flip with 50% probability\n",
        "    if random.random() < 0.5:\n",
        "        image = TF.hflip(image)\n",
        "        mask = TF.hflip(mask)\n",
        "\n",
        "    if image.shape[0] != 1:\n",
        "        image = image[:3, :, :]  # Assumes that first three channels are RGB\n",
        "        mask = mask.repeat(3, 1, 1)\n",
        "        image = TF.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    else:\n",
        "        image = TF.normalize(image, mean=0.445, std=0.269)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def denormalize(image_tensor):\n",
        "    \"\"\"\n",
        "    Denormalizes an image tensor back to its original range for visualization purposes.\n",
        "\n",
        "    Args:\n",
        "    - image_tensor (torch.Tensor): Input image tensor to be denormalized.\n",
        "\n",
        "    Returns:\n",
        "    - torch.Tensor: Denormalized image tensor with pixel values in the original range.\n",
        "    \"\"\"\n",
        "\n",
        "    if image_tensor.shape[0] != 1:\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(image_tensor.device)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(image_tensor.device)\n",
        "    else:\n",
        "        mean = torch.tensor([0.5]).view(1, 1, 1).to(image_tensor.device)\n",
        "        std = torch.tensor([0.5]).view(1, 1, 1).to(image_tensor.device)\n",
        "    image_tensor = image_tensor * std + mean\n",
        "    return image_tensor\n",
        "\n",
        "\n",
        "\n",
        "def segmentation_dataset(data_path, transform=None):\n",
        "  \"\"\"\n",
        "  Custom dataset loader for segmentation tasks.\n",
        "\n",
        "  Args:\n",
        "  - data_path (str): Path to the directory containing 'images' and 'masks' subdirectories.\n",
        "  - transform (callable, optional): Optional transform to be applied to the image and mask pairs.\n",
        "\n",
        "  Returns:\n",
        "  - list: List of tuples (image, mask), where image and mask are numpy arrays read from .tif files.\n",
        "\n",
        "  Notes:\n",
        "  - Expects the directory structure:\n",
        "    - data_path/\n",
        "      - images/\n",
        "        - *.tif (panchromatic or RGB images)\n",
        "      - masks/\n",
        "        - *.tif (corresponding masks)\n",
        "  \"\"\"\n",
        "\n",
        "   images_path = os.path.join(data_path, \"images\")\n",
        "   masks_path = os.path.join(data_path, \"masks\")\n",
        "   image_files = os.listdir(images_path)\n",
        "\n",
        "   dataset = []\n",
        "   for img_name in image_files:\n",
        "         image = tifffile.imread(os.path.join(images_path, img_name)) #read .tif file\n",
        "         mask = tifffile.imread(os.path.join(masks_path, img_name[:-4] + '.tif')) #read corresponding mask .tif file\n",
        "\n",
        "         if transform:\n",
        "           image, mask = trans(image, mask)\n",
        "           #mask = transform(mask) #apply transform to both image and corresponding mask\n",
        "\n",
        "         dataset.append((image, mask)) #append the image-mask pair in the dataset\n",
        "   return dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#https://github.com/hsiangyuzhao/Segmentation-Metrics-PyTorch/blob/master/metric.py\n",
        "def metrics(groundtruths, predictions, threshold = 0.5):\n",
        "    \"\"\"\n",
        "    Computes evaluation metrics for binary segmentation tasks based on ground truth and predicted tensors.\n",
        "\n",
        "    Args:\n",
        "    - groundtruths (list of torch.Tensor): List of ground truth tensors containing binary labels (0 or 1).\n",
        "    - predictions (list of torch.Tensor): List of prediction tensors containing probabilities (0 to 1).\n",
        "    - threshold (float, optional): Threshold value for binarizing predictions. Default is 0.5.\n",
        "\n",
        "    Returns:\n",
        "    - float: Pixel accuracy metric.\n",
        "    - float: Dice coefficient metric.\n",
        "    - float: Precision metric.\n",
        "    - float: Recall metric.\n",
        "    - float: Specificity metric.\n",
        "\n",
        "    Notes:\n",
        "    - Each tensor in groundtruths and predictions should have the same shape.\n",
        "    - Metrics are computed based on summed values across all tensors in groundtruths and predictions.\n",
        "    - Uses torch.Tensor operations for calculation.\n",
        "    - Prints a formatted table of computed metrics for easy visualization.\n",
        "    \"\"\"\n",
        "\n",
        "    output = torch.cat(groundtruths, dim = 0)\n",
        "    target = torch.cat(predictions, dim = 0)\n",
        "    #pred = (all_outputs > threshold).float()\n",
        "\n",
        "    #output = predictions.view(-1, )\n",
        "    #target = groundtruths.view(-1, ).float()\n",
        "\n",
        "    tp = torch.sum(output * target)  # TP\n",
        "    fp = torch.sum(output * (1 - target))  # FP\n",
        "    fn = torch.sum((1 - output) * target)  # FN\n",
        "    tn = torch.sum((1 - output) * (1 - target))  # TN\n",
        "\n",
        "    eps = 1e-7 # Small number to avoid devision by zero\n",
        "\n",
        "    pixel_acc = (tp + tn + eps) / (tp + tn + fp + fn + eps)\n",
        "    dice = (2 * tp + eps) / (2 * tp + fp + fn + eps)\n",
        "    precision = (tp + eps) / (tp + fp + eps)\n",
        "    recall = (tp + eps) / (tp + fn + eps)\n",
        "    specificity = (tn + eps) / (tn + fp + eps)\n",
        "\n",
        "    table = [[\"Pixel Accuracy\", pixel_acc],\n",
        "              [\"Dice\", dice],\n",
        "              [\"Precision\", precision],\n",
        "              [\"Recall\", recall],\n",
        "              [\"Specificity\", specificity]]\n",
        "\n",
        "    head = [\"Metric\", \"Value\"]\n",
        "\n",
        "    print(tabulate(table, headers=head, tablefmt=\"grid\"))\n",
        "\n",
        "    return pixel_acc, dice, precision, recall, specificity\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    \"\"\"\n",
        "    Get the current learning rate of the optimizer.\n",
        "\n",
        "    Args:\n",
        "    - optimizer (torch.optim.Optimizer): The optimizer object from which to retrieve the learning rate.\n",
        "\n",
        "    Returns:\n",
        "    - float: The current learning rate used by the optimizer.\n",
        "\n",
        "    Notes:\n",
        "    - Assumes that the optimizer is initialized and contains at least one parameter group.\n",
        "    \"\"\"\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "alpha = 0.35\n",
        "beta = 0.65\n",
        "# Tversky loss https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch\n",
        "class TverskyLoss(nn.Module):\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "      \"\"\"\n",
        "      Tversky Loss for binary segmentation.\n",
        "\n",
        "      Args:\n",
        "      - alpha (float): Weight of false positives (FP). Default is 0.3.\n",
        "      - beta (float): Weight of false negatives (FN). Default is 0.7.\n",
        "      - smooth (float): Smoothing factor to avoid division by zero. Default is 1.\n",
        "\n",
        "      Notes:\n",
        "      - Assumes inputs are logits or raw outputs from the model (before applying sigmoid).\n",
        "      \"\"\"\n",
        "      super(TverskyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1, alpha=alpha, beta=beta):\n",
        "      \"\"\"\n",
        "      Compute Tversky loss between inputs (predictions) and targets (ground truth).\n",
        "\n",
        "      Args:\n",
        "      - inputs (torch.Tensor): Predicted logits or raw outputs from the model.\n",
        "      - targets (torch.Tensor): Ground truth binary labels (0 or 1).\n",
        "\n",
        "      Returns:\n",
        "      - torch.Tensor: Computed Tversky loss.\n",
        "\n",
        "      Notes:\n",
        "      - If your model already applies a sigmoid or equivalent activation, comment out the `F.sigmoid(inputs)` line.\n",
        "      \"\"\"\n",
        "\n",
        "      #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "      inputs = F.sigmoid(inputs)\n",
        "\n",
        "      #flatten label and prediction tensors\n",
        "      inputs = inputs.view(-1)\n",
        "      targets = targets.view(-1)\n",
        "\n",
        "      #True Positives, False Positives & False Negatives\n",
        "      TP = (inputs * targets).sum()\n",
        "      FP = ((1-targets) * inputs).sum()\n",
        "      FN = (targets * (1-inputs)).sum()\n",
        "\n",
        "      Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)\n",
        "\n",
        "      return 1 - Tversky"
      ],
      "metadata": {
        "id": "kttEHLCcd_We"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image processing\n",
        "Both panchromatic and multispectral satellite images can be processed in this code. Only panchromatic images are used, and multispectral lines are commented out to save processing time.\n",
        "If multispectral images are processed, make sure the sections with pansharpened images are used."
      ],
      "metadata": {
        "id": "9nAlB2Z0fx3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define paths and create directories"
      ],
      "metadata": {
        "id": "0HKyL7G0iDLN"
      }
    },
    {
      "metadata": {
        "id": "d7_JzYKZMUEY"
      },
      "cell_type": "code",
      "source": [
        "input_path = os.path.join(project_path, 'data/1_preprocessing/input')\n",
        "os.makedirs(input_path, exist_ok=True)\n",
        "output_path =  os.path.join(project_path, 'data/1_preprocessing/output')\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "panchromatic_path = os.path.join(project_path, 'data/1_preprocessing/1ps_image')\n",
        "os.makedirs(panchromatic_path, exist_ok=True)\n",
        "panchromatic = os.path.join(panchromatic_path, '22MAR25134903-P3DS-014983717010_01_P001.TIF')\n",
        "\n",
        "shapefile_path  = os.path.join(project_path, 'data/1_preprocessing/1ps_shapefile')\n",
        "os.makedirs(shapefile_path, exist_ok=True)\n",
        "\n",
        "panchromatic_reduced_path = os.path.join(project_path, 'data/1_preprocessing/1a_reduced_image/')\n",
        "os.makedirs(panchromatic_reduced_path, exist_ok=True)\n",
        "panchromatic_reduced = os.path.join(panchromatic_reduced_path, '1a_panchromatic_reduced.TIF')\n",
        "\n",
        "panchromatic_parts_path = os.path.join(project_path, 'data/1_preprocessing/1b_panchromatic_parts')\n",
        "os.makedirs(panchromatic_parts_path, exist_ok=True)\n",
        "\n",
        "mask_path = os.path.join(project_path, 'data/1_preprocessing/1c_raster_mask')\n",
        "os.makedirs(mask_path, exist_ok=True)\n",
        "mask = os.path.join(mask_path, '1c_raster_mask.TIF')\n",
        "\n",
        "mask_parts_path = os.path.join(project_path, 'data/1_preprocessing/1d_mask_parts')\n",
        "os.makedirs(mask_parts_path, exist_ok=True)\n",
        "\n",
        "csv_path = os.path.join(project_path, 'data/1_preprocessing/1e_raster_statistics')\n",
        "os.makedirs(csv_path, exist_ok=True)\n",
        "csv = os.path.join(csv_path, '1e_raster_statistics.csv')\n",
        "\n",
        "output_path_images = os.path.join(project_path, 'data/1_preprocessing/1f_divided_images')\n",
        "os.makedirs(output_path_images, exist_ok=True)\n",
        "\n",
        "output_path_masks = os.path.join(project_path, 'data/1_preprocessing/1g_divided_masks')\n",
        "os.makedirs(output_path_masks, exist_ok=True)\n",
        "\n",
        "# Define paths for deep learning input\n",
        "training_set_dl = os.path.join(project_path, 'data/2_deep_learning/2a_training_set')\n",
        "\n",
        "# Create directories\n",
        "for path in training_set_dl:\n",
        "    os.makedirs(os.path.join(path, 'train', 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(path, 'train', 'masks'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(path, 'val', 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(path, 'val', 'masks'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(path, 'test', 'images'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(path, 'test', 'masks'), exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reduce the resolution and save to a new folder\n",
        "\n",
        "Read the raster image, reduce the radiometric resolution to 8 bits, and save the output in a new folder."
      ],
      "metadata": {
        "id": "G1Yftx9giJef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_radiometric_resolution(panchromatic, panchromatic_reduced)"
      ],
      "metadata": {
        "id": "TC_tjP1jbmjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create masks from annotated seals and save to a new folder"
      ],
      "metadata": {
        "id": "uAShwpKXuHZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a 5x5 mask from the annotated seals with the current raster reference for pixels."
      ],
      "metadata": {
        "id": "CJYZTWmnomQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_raster_with_polygon(panchromatic_reduced, raster_points_to_polygons(panchromatic_reduced, shapefile_path), mask)"
      ],
      "metadata": {
        "id": "2a1haNHB9bZE"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tile the raster and mask into smaller images"
      ],
      "metadata": {
        "id": "-EHTTA0IuYp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the images with Imagenet they have to be tiled into 224x224 pixel tiles. The tile_raster function takes care of this and stores the result in the final parameter."
      ],
      "metadata": {
        "id": "WagQWgr0sQfk"
      }
    },
    {
      "metadata": {
        "id": "eFROUVuKMUEm"
      },
      "cell_type": "code",
      "source": [
        "width = 224\n",
        "height = 224\n",
        "\n",
        "tile_raster(panchromatic_reduced, width, height, panchromatic_parts_path)\n",
        "tile_raster(mask, width, height, mask_parts_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove images and masks from a directory if the raster contains zero value pixels.\n",
        "Remove any rasters and their corresponding masks if there are 0 values in the raster. This way, tiled rasters at the edge of the image are removed and thus excluded from training.\n"
      ],
      "metadata": {
        "id": "7yJLTMDUvLd1"
      }
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "remove_zero_raster_mask(panchromatic_parts_path, mask_parts_path)"
      ],
      "metadata": {
        "id": "kv00NfgBMUEo"
      },
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Update mask image values\n",
        "Update the masks based on corresponding panchromatic images, setting mask pixels to 0 where the panchromatic pixel value exceeds the threshold to more resemble a seal shape. This effectively removes the ice pixels from the mask."
      ],
      "metadata": {
        "id": "hqx2NMUevYTL"
      }
    },
    {
      "metadata": {
        "id": "vg10uVXTMUEt"
      },
      "cell_type": "code",
      "source": [
        "threshold_value = 70\n",
        "update_masks(panchromatic_parts_path, mask_parts_path, threshold_value)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Organize the images based on the number of seals\n",
        "Organize images into subfolders based on the number of seals and the average pixel values specified in a CSV file.\n",
        "Obtain_statistics commented out due to long processing time. If the .csv file exists, do not run.\n",
        "\n",
        "Split the data into training, validation, and test sets based on ratios defined in the function, and organize the images and their corresponding masks into separate folders."
      ],
      "metadata": {
        "id": "F6Z6xcO7w9eJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Only run if .csv file does not exist yet\n",
        "obtain_statistics(panchromatic_parts_path, shapefile_path, csv_path)"
      ],
      "metadata": {
        "id": "PE_BhCdQMUEv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "organize_images(csv_path, panchromatic_parts_path, output_panchromatic)\n",
        "organize_images(csv_path, mask_parts_path, output_mask)\n",
        "\n",
        "split_data(output_panchromatic, output_mask, panchromatic_path_dl)"
      ],
      "metadata": {
        "id": "IL0D69vcFMwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the SealNN model"
      ],
      "metadata": {
        "id": "gWvrf4W_XsaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### File paths\n",
        "Define the file paths to training, validation and test data.\n",
        "Choose RGB instead if using multispectral images."
      ],
      "metadata": {
        "id": "n2N9OagrySF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose RGB or BW\n",
        "col = \"panchromatic\" # BW\n",
        "# col = \"pansharpened\" # RGB\n",
        "\n",
        "train_data_path = \"/content/drive/MyDrive/TheNavySeals/data/\"+ col + \"/train\" #TODO: change to relative file path\n",
        "val_data_path = \"/content/drive/MyDrive/TheNavySeals/data/\"+ col + \"/val\"\n",
        "test_data_path = \"/content/drive/MyDrive/TheNavySeals/data/\"+ col + \"/test\""
      ],
      "metadata": {
        "id": "prsi-JqipbHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset preparation and creating dataloaders\n",
        "Setting up the training, validation, and testing datasets using the segmentation_dataset function, applies transformations (horizontal flip and normalization for the pretrained Imagenet model), and creating DataLoader objects for batching and shuffling the data."
      ],
      "metadata": {
        "id": "aWHGGr6Fy3sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test datasets with augmentation (horizontal flip & normalization)\n",
        "train_dataset = segmentation_dataset(train_data_path, transform=trans)\n",
        "val_dataset = segmentation_dataset(val_data_path, transform=trans)\n",
        "test_dataset = segmentation_dataset(test_data_path, transform=trans)\n",
        "\n",
        "batch_size = 8 #Can be changed later\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Create train and test data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "B3GOKKgAprdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the data type, mean and std"
      ],
      "metadata": {
        "id": "RyIk5TcH0SV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, mask = train_dataset[130]\n",
        "print(image.shape, type(image))\n",
        "print(mask.shape, type(image))\n",
        "print(len(train_dataset), len(val_dataset), len(test_dataset))\n",
        "print(f'STD: {torch.std(image)}, Mean: {torch.mean(image)}')"
      ],
      "metadata": {
        "id": "ek34vmuHptNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing an example image and mask"
      ],
      "metadata": {
        "id": "8s5GNHQA0Y2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, mask = train_dataset[125]\n",
        "channels = image.shape[0]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axes[0].imshow(denormalize(image).permute(1,2,0), cmap = 'gray')  # Convert from tensor format (C, H, W) to (H, W, C)\n",
        "axes[0].set_title('Image')\n",
        "axes[0].axis('off')\n",
        "axes[1].imshow(mask.permute(1,2,0), cmap = 'gray')  # Convert from tensor format (C, H, W) to (H, W, C)\n",
        "axes[1].set_title('Mask')\n",
        "axes[1].axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f'STD: {torch.std(image)}, Mean: {torch.mean(image)}, Min: {torch.min(image)}, Max: {torch.max(image)}')\n",
        "print(image.shape)"
      ],
      "metadata": {
        "id": "IAtm0BI1pu1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the GPU colab assigns to you"
      ],
      "metadata": {
        "id": "x8H1e36x0fX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "zwA-Rsq7pwo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### U-Net\n",
        "Initializing the pretrained U-Net model using the segmentation_models.pytorch library (smp).\n",
        "Source:https://smp.readthedocs.io/en/latest/models.html#unet"
      ],
      "metadata": {
        "id": "SOMFZhPU0sdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = smp.Unet(\n",
        "    encoder_name=\"efficientnet-b7\",         # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
        "    encoder_weights='imagenet',             # imagenet if using pretrained, None is no pretraining\n",
        "    in_channels=channels,                   # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
        "    classes=channels                        # model output channels (number of classes in your dataset)\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "EHmYzQ09pyaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The training loop\n",
        "Setting up the training loop with the following parameters:\n",
        "\n",
        "\n",
        "*   Learning rate : 0.003\n",
        "*   Number of epochs : 30\n",
        "*   Loss function : Tversky loss (alpha and beta defined in the function)\n",
        "*   Optimizer : Adam\n",
        "*   Scheduler : ReduceLROnPlateau (factor : 0.1, patience : 5)\n",
        "\n",
        "Plot the training and validation loss at the end.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p5vhYOCZ1Kzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.003\n",
        "num_epochs = 30\n",
        "\n",
        "criterion = TverskyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "# Training and validation loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "best_loss = 10000000\n",
        "best_model = 0\n",
        "best_epoch = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time() #start time for epoch\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    tr_all_outputs = []\n",
        "    tr_all_masks = []\n",
        "    for image, mask in train_loader:\n",
        "        image, mask = image.to(device, dtype=torch.float), mask.to(device, dtype=torch.float)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(image)\n",
        "        loss = criterion(outputs, mask)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        outputs= outputs.cpu().detach()\n",
        "        tr_all_outputs.append(outputs)\n",
        "        tr_all_masks.append(mask.cpu().detach())\n",
        "        train_loss += loss.item()\n",
        "\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_all_outputs = []\n",
        "    val_all_masks = []\n",
        "    with torch.no_grad():\n",
        "        for image, mask in val_loader:\n",
        "            image, mask = image.to(device, dtype=torch.float), mask.to(device, dtype=torch.float)\n",
        "            outputs = model(image)\n",
        "            loss = criterion(outputs, mask)\n",
        "            outputs = torch.sigmoid(outputs)\n",
        "            outputs= outputs.cpu().detach()\n",
        "            val_all_outputs.append(outputs)\n",
        "            val_all_masks.append(mask.cpu().detach())\n",
        "            val_loss += loss.item()\n",
        "\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "      best_loss = val_loss\n",
        "      best_model = model\n",
        "      best_epoch = epoch+1\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Epoch time: {epoch_time: .2f}s, LR: {get_lr(optimizer)}')\n",
        "print(f\"Best epoch is {best_epoch}, with a validation loss of {best_loss}\")\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Val Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GEMJv_wgp13d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate the trained model\n",
        "Show pixel accuracy, dice, precision, recall and specificity to evaluate the model metrics.\n",
        "Plot the precision-recall curve."
      ],
      "metadata": {
        "id": "I-Pckd9p3-vW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model and collect outputs and masks\n",
        "best_model.eval()\n",
        "\n",
        "te_all_outputs = []\n",
        "te_all_masks = []\n",
        "\n",
        "for image, mask in test_loader:\n",
        "    image, mask = image.to(device, dtype=torch.float), mask.to(device, dtype=torch.float)\n",
        "    outputs = best_model(image)\n",
        "\n",
        "    outputs = torch.sigmoid(outputs)  # Assuming the output is a logit\n",
        "    outputs = outputs.cpu().detach()\n",
        "\n",
        "    te_all_outputs.append(outputs)\n",
        "    te_all_masks.append(mask.cpu().detach())\n",
        "\n",
        "\n",
        "metrics(te_all_outputs, te_all_masks)\n",
        "\n",
        "te_all_outputs = torch.cat(te_all_outputs).view(-1)\n",
        "te_all_masks = torch.cat(te_all_masks).view(-1)\n",
        "\n",
        "\n",
        "te_all_outputs_np = te_all_outputs.numpy()\n",
        "te_all_masks_np = te_all_masks.numpy()\n",
        "\n",
        "# Calculate precision and recall for different thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(te_all_masks_np, te_all_outputs_np)\n",
        "\n",
        "# Plot precision-recall curve\n",
        "plt.figure()\n",
        "plt.plot(recall, precision, marker=' ')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fCj_Pe37p7i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize test images\n",
        "Plot all the test images with their corresponding true and predicted mask for visual evaluation.\n",
        "Reduce the number of visualized test data if the number of test images is high.\n"
      ],
      "metadata": {
        "id": "M5G_OhLj4zIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index of the image you want to visualize\n",
        "for i in range(len(test_dataset)):\n",
        "\n",
        "# Set visualization variables\n",
        "  image, mask = test_dataset[i]\n",
        "  image = image.to(device, dtype=torch.float).unsqueeze(0)  # Add batch dimension\n",
        "  mask = mask.to(device, dtype=torch.float).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "  best_model.eval()\n",
        "  with torch.no_grad():\n",
        "    pred = best_model(image)\n",
        "\n",
        "  pred = torch.sigmoid(pred).squeeze(0).cpu().numpy()  # Convert prediction to numpy array\n",
        "  pred_print = (pred == 1).astype(float)\n",
        "\n",
        "  fig, axes = plt.subplots(1, 3, figsize=(6, 2))\n",
        "\n",
        "  image_np = denormalize(image.squeeze()).cpu().numpy()\n",
        "  axes[0].imshow(image_np.transpose(1, 2, 0), cmap = 'gray')  # Use grayscale if single channel\n",
        "  axes[0].set_title('Image')\n",
        "  axes[0].axis('off')\n",
        "\n",
        "  mask_np = mask.squeeze().cpu().numpy()\n",
        "  #print(f'True mask shape for imshow: {mask_np.shape}')\n",
        "  if channels != 1:\n",
        "    axes[1].imshow(mask_np.transpose(1, 2, 0), cmap = 'gray')  # Convert mask to numpy array\n",
        "  else:\n",
        "    axes[1].imshow(mask_np, cmap = 'gray')\n",
        "\n",
        "  axes[1].set_title('True Mask')\n",
        "  axes[1].axis('off')\n",
        "\n",
        "  #print(f'Predicted mask shape for imshow: {pred_print.shape}')\n",
        "  axes[2].imshow(pred_print.transpose(1, 2, 0), cmap = 'gray')  # Use grayscale colormap for binary mask\n",
        "  axes[2].set_title('Predicted Mask')\n",
        "  axes[2].axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y-aDjPOMp-tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the model weigths\n",
        "Save the weights and use them in post-processing."
      ],
      "metadata": {
        "id": "P0C0rXEe3ht1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this code if your model is good, you can save the weights to drive:\n",
        "if path.exists(\"/content/drive/MyDrive/TheNavySeals/models\") == False:\n",
        "  os.mkdir(\"/content/drive/MyDrive/TheNavySeals/models\")\n",
        "\n",
        "model_name = \"SealNN\"\n",
        "model_path = \"/content/drive/MyDrive/TheNavySeals/models/\"+ model_name\n",
        "torch.save(best_model, model_path)"
      ],
      "metadata": {
        "id": "bnROhknqp4A0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}